[{"UID":"1","abstract":"Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term Graph Network-based Simulators (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.","authors":["Alvaro Sanchez-Gonzalez","Jonathan Godwin","Tobias Pfaff","Rex Ying","Jure Leskovec","Peter W. Battaglia"],"citation":"@inproceedings{DBLP:conf/icml/Sanchez-Gonzalez20,\n  author    = {Alvaro Sanchez{-}Gonzalez and\n               Jonathan Godwin and\n               Tobias Pfaff and\n               Rex Ying and\n               Jure Leskovec and\n               Peter W. Battaglia},\n  title     = {Learning to Simulate Complex Physics with Graph Networks},\n  booktitle = {Proceedings of the 37th International Conference on Machine Learning,\n               {ICML} 2020, 13-18 July 2020, Virtual Event},\n  series    = {Proceedings of Machine Learning Research},\n  volume    = {119},\n  pages     = {8459--8468},\n  publisher = {{PMLR}},\n  year      = {2020},\n  url       = {http://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},\n  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},\n  biburl    = {https://dblp.org/rec/conf/icml/Sanchez-Gonzalez20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","code_link":"","data_link":"","date":"14/9/2020","keywords":["Graph Neural Networks","forward simulation","GNS","GNNs","message passing","encoder-processor-decoder","physical simulation"],"nickname":"","pdf_url":"https://arxiv.org/pdf/2002.09405.pdf","project_link":"","talk_link":"","title":"Learning to Simulate Complex Physics with Graph Networks","venue":"ICML","year":"2020"},{"UID":"2","abstract":"Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.","authors":["Tobias Pfaff","Meire Fortunato","Alvaro Sanchez-Gonzalez","Peter W. Battaglia"],"citation":"@inproceedings{DBLP:conf/iclr/PfaffFSB21,\n  author    = {Tobias Pfaff and\n               Meire Fortunato and\n               Alvaro Sanchez{-}Gonzalez and\n               Peter W. Battaglia},\n  title     = {Learning Mesh-Based Simulation with Graph Networks},\n  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,\n               Virtual Event, Austria, May 3-7, 2021},\n  publisher = {OpenReview.net},\n  year      = {2021},\n  url       = {https://openreview.net/forum?id=roNqYL0\\_XP},\n  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},\n  biburl    = {https://dblp.org/rec/conf/iclr/PfaffFSB21.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","code_link":"","data_link":"","date":"18/6/2021","keywords":["MeshGraphNets","Graph Neural Networks","forward simulation","GNNs","message passing","adaptive remeshing","encoder-processor-decoder","physical simulation","runtime efficiency","neural network simulators"],"nickname":"","pdf_url":"https://arxiv.org/pdf/2010.03409.pdf","project_link":"","talk_link":"","title":"Learning Mesh-Based Simulation with Graph Networks","venue":"ICLR","year":"2021"},{"UID":"3","abstract":"In recent years, there has been a growing interest in using machine learning to overcome the high cost of numerical simulation, with some learned models achieving impressive speed-ups over classical solvers whilst maintaining accuracy. However, these methods are usually tested at low-resolution settings, and it remains to be seen whether they can scale to the costly high-resolution simulations that we ultimately want to tackle. In this work, we propose two complementary approaches to improve the framework from MeshGraphNets, which demonstrated accurate predictions in a broad range of physical systems. MeshGraphNets relies on a message passing graph neural network to propagate information, and this structure becomes a limiting factor for high-resolution simulations, as equally distant points in space become further apart in graph space. First, we demonstrate that it is possible to learn accurate surrogate dynamics of a high-resolution system on a much coarser mesh, both removing the message passing bottleneck and improving performance; and second, we introduce a hierarchical approach (MultiScale MeshGraphNets) which passes messages on two different resolutions (fine and coarse), significantly improving the accuracy of MeshGraphNets while requiring less computational resources.","authors":["Meire Fortunato","Tobias Pfaff","Peter Wirnsberger","Alexander Pritzel","Peter Battaglia"],"citation":"@inproceedings{fortunato2022multiscale,\n  title={MultiScale MeshGraphNets},\n  author={Fortunato, Meire and Pfaff, Tobias and Wirnsberger, Peter and Pritzel, Alexander and Battaglia, Peter},\n  booktitle={ICML 2022 2nd AI for Science Workshop},\n  year={2022}\n}","code_link":"","data_link":"","date":"15/6/2022","keywords":["Multiscale MeshGraphNets","Graph Neural Networks","GNNs","message passing","encoder-processor-decoder","computational efficiency","neural network simulators","computational fluid dynamics","CFD simulation","COMSOL","CylinderFlow"],"nickname":"","pdf_url":"https://openreview.net/pdf?id=G3TRIsmMhhf","project_link":"","talk_link":"","title":"MultiScale MeshGraphNets","venue":"ICML-AI4Science","year":"2022"},{"UID":"4","abstract":"Graph-based next-step prediction models have recently been very successful in modeling complex high-dimensional physical systems on irregular meshes. However, due to their short temporal attention span, these models suffer from error accumulation and drift. In this paper, we propose a new method that captures long-term dependencies through a transformer-style temporal attention model. We introduce an encoder-decoder structure to summarize features and create a compact mesh representation of the system state, to allow the temporal model to operate on a low-dimensional mesh representations in a memory efficient manner. Our method outperforms a competitive GNN baseline on several complex fluid dynamics prediction tasks, from sonic shocks to vascular flow. We demonstrate stable rollouts without the need for training noise and show perfectly phase-stable predictions even for very long sequences. More broadly, we believe our approach paves the way to bringing the benefits of attention-based sequence models to solving high-dimensional complex physics tasks.","authors":["Xu Han","Han Gao","Tobias Pfaff","Jian-Xun Wang","Li-Ping Liu"],"citation":"@article{DBLP:journals/corr/abs-2201-09113,\n  author    = {Xu Han and\n               Han Gao and\n               Tobias Pffaf and\n               Jian{-}Xun Wang and\n               Li{-}Ping Liu},\n  title     = {Predicting Physics in Mesh-reduced Space with Temporal Attention},\n  journal   = {CoRR},\n  volume    = {abs/2201.09113},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2201.09113},\n  eprinttype = {arXiv},\n  eprint    = {2201.09113},\n  timestamp = {Tue, 01 Feb 2022 14:59:01 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-09113.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","code_link":"","data_link":"","date":"26/5/2022","keywords":["Graph Neural Networks","transformers","autoregressive","multi-head attention","temporal attention","GNNs","message passing","encoder-processor-decoder","computational efficiency","neural network simulators","computational fluid dynamics","CFD simulation","cylinderflow","mesh","graph mesh reducer","autoencoder"],"nickname":"","pdf_url":"https://arxiv.org/pdf/2201.09113.pdf","project_link":"","talk_link":"","title":"Predicting Physics in Mesh-reduced Space with Temporal Attention","venue":"ICLR","year":"2022"},{"UID":"5","abstract":"Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.","authors":["Filipe de Avila Belbute-Peres","Thomas D. Economon","J. Zico Kolter"],"citation":"@inproceedings{DBLP:conf/icml/Belbute-PeresEK20,\n  author    = {Filipe de Avila Belbute{-}Peres and\n               Thomas D. Economon and\n               J. Zico Kolter},\n  title     = {Combining Differentiable {PDE} Solvers and Graph Neural Networks for\n               Fluid Flow Prediction},\n  booktitle = {Proceedings of the 37th International Conference on Machine Learning,\n               {ICML} 2020, 13-18 July 2020, Virtual Event},\n  series    = {Proceedings of Machine Learning Research},\n  volume    = {119},\n  pages     = {2402--2411},\n  publisher = {{PMLR}},\n  year      = {2020},\n  url       = {http://proceedings.mlr.press/v119/de-avila-belbute-peres20a.html},\n  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},\n  biburl    = {https://dblp.org/rec/conf/icml/Belbute-PeresEK20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","code_link":"","data_link":"","date":"16/8/2020","keywords":["Graph Neural Networks","GNNs","Graph Convolutional Networks","GCNs","message passing","hybrid simulator","computational efficiency","SU2","computational fluid dynamics","CFD simulation","mesh","adjoint-based differentiation","differentiable simulator"],"nickname":"","pdf_url":"https://arxiv.org/pdf/2007.04439.pdf","project_link":"","talk_link":"","title":"Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction","venue":"ICML","year":"2020"},{"UID":"6","abstract":"The original Seven Motifs set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the Nine Motifs of Simulation Intelligence, a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.","authors":["Alexander Lavin","Hector Zenil","Brooks Paige","David Krakauer","Justin Gottschlich","Tim Mattson","Anima Anandkumar","Sanjay Choudry","Kamil Rocki","At\u0131l\u0131m G\u00fcne\u015f Baydin","Carina Prunkl","Brooks Paige","Olexandr Isayev","Erik Peterson","Peter L. McMahon","Jakob Macke","Kyle Cranmer","Jiaxin Zhang","Haruko Wainwright","Adi Hanuka","Manuela Veloso","Samuel Assefa","Stephan Zheng","Avi Pfeffer"],"citation":"@article{DBLP:journals/corr/abs-2112-03235,\n  author    = {Alexander Lavin and\n               Hector Zenil and\n               Brooks Paige and\n               David Krakauer and\n               Justin Gottschlich and\n               Tim Mattson and\n               Anima Anandkumar and\n               Sanjay Choudry and\n               others},\n  title     = {Simulation Intelligence: Towards a New Generation of Scientific Methods},\n  journal   = {CoRR},\n  volume    = {abs/2112.03235},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2112.03235},\n  eprinttype = {arXiv},\n  eprint    = {2112.03235},\n  timestamp = {Mon, 03 Jan 2022 22:03:29 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-03235.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}","code_link":"","data_link":"","date":"6/12/2021","keywords":["simulation intelligence","review","simulation overview","simulation-based inference","differentiable programming","probabilistic programming","causal modeling","surrogate modeling","emulation","nine motifs","multi-physics modeling","multi-scale modeling"],"nickname":"","pdf_url":"https://arxiv.org/pdf/2112.03235.pdf","project_link":"","talk_link":"","title":"Simulation Intelligence: Towards a New Generation of Scientific Methods","venue":"arXiv","year":"2021"}]
